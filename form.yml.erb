<%-
  groups = OodSupport::User.new.groups.sort_by(&:id).tap { |groups|
    groups.unshift(groups.delete(OodSupport::Process.group))
  }.map(&:name).grep(/.*[0-9]{5}/)
  all_groups = OodSupport::User.new.groups.sort_by(&:id).tap { |groups|
    groups.unshift(groups.delete(OodSupport::Process.group))
  }.map(&:name)
  
  has_slurm_apps = all_groups.include?("nesi-staff") || all_groups.include?("ondemand-slurm-apps")
  
  restricted_projects = ["nesi02659", "nesi99991"]
-%>

---
description: Request VS Code Server
submit: submit.yml.erb
title: VS Code

form:
  - cluster
  - account 
  - codeserver
  - bc_num_hours
  - cores
  - memory
  - node

attributes:
  # select the cluster
  cluster:
    widget: "select"
    options:
      - [
          "Kubernetes", "my-k8s-cluster",
          data-hide-gpu: true,
          data-set-gpu: 'none',
        ]
<% if has_slurm_apps %>
      - ["Slurm HPC", "nesi_tdc_hpc"]
    help: |
      Kubernetes jobs are more likely to start promptly - "Slurm HPC" jobs may have to wait in the queue.
      If you need Apptainer or GPUs then you must choose "Slurm HPC" currently.
<% end %>
    cacheable: false  # we always want k8s on top as the default?
    
  account:    # Add this section
    label: "Project Code"
    widget: select
    options:
      <%- groups.each do |group| %>
      - [
          "<%= group %>",       # visible label
          "<%= group %>",       # value submitted
          <% if restricted_projects.include?(group) %>
          data-hide-memory: true,
          data-set-memory: 4,
          data-hide-cores: true,
          data-set-cores: 2
          <% end %>
        ]
      <%- end %>
    required: true
    display: true

  codeserver:
    widget: select
    label: "VS Code Version"
    options:
      - [ "4.100.2", "code-server/4.100.2" ]
      - [ "4.92.2", "code-server/4.92.2" ]
  node: ""
  bc_queue:
    value: interactive
  bc_num_hours:
    max: 8
    min: 1
    step: 1
    value: 1
    widget: number_field
  cores:
    label: Number of cores
    max: 16
    min: 1
    value: 2
    widget: number_field
  memory:
    label: Memory per job (GB)
    max: 32
    min: 1
    step: 1
    value: 1
    widget: number_field
  gpu:
    label: GPU
    widget: select
    options:
      - 'none'
      - [ "L4 (24 GB VRAM, aimed at AI workloads, up to single precision)", 'L4' ]
      - [ "H100 (96 GB VRAM, recommend submitting as Slurm jobs due to potential wait times)", "H100" ]
    help: |
      **WARNING**: selecting a GPU can significantly increase the time it takes for your session to start!
    display: true
